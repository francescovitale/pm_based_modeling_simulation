{"cells":[{"cell_type":"markdown","source":["# **Anchor Google Drive file system**"],"metadata":{"id":"Ton_Be17T8XV"},"id":"Ton_Be17T8XV"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r0Hmv1ChT738","executionInfo":{"status":"ok","timestamp":1760545493503,"user_tz":-120,"elapsed":18798,"user":{"displayName":"Francesco Vitale","userId":"03232356734689144291"}},"outputId":"d67250ef-9ca0-443a-97b1-983390e4f2a2","collapsed":true},"id":"r0Hmv1ChT738","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","id":"1586dc85","metadata":{"id":"1586dc85"},"source":["# **Import libraries**"]},{"cell_type":"code","execution_count":2,"id":"16d8775d","metadata":{"id":"16d8775d","executionInfo":{"status":"ok","timestamp":1760545500110,"user_tz":-120,"elapsed":5131,"user":{"displayName":"Francesco Vitale","userId":"03232356734689144291"}}},"outputs":[],"source":["import os, re, argparse, json\n","from pathlib import Path\n","import numpy as np\n","import pandas as pd\n","import sys\n","from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import joblib\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm\n","import random\n","from sklearn.metrics import f1_score\n","from sklearn.preprocessing import StandardScaler\n","from collections import defaultdict\n","from concurrent.futures import ThreadPoolExecutor"]},{"cell_type":"markdown","source":["# **Global definitions**"],"metadata":{"id":"nf9icaNaDKDF"},"id":"nf9icaNaDKDF"},{"cell_type":"code","source":["input_dir = \"/content/drive/MyDrive/AD_PM_ROAD/Input/\"\n","output_dir = \"/content/drive/MyDrive/AD_PM_ROAD/Output/\"\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"metadata":{"id":"m3vrJgUDDbWM","executionInfo":{"status":"ok","timestamp":1760545501131,"user_tz":-120,"elapsed":10,"user":{"displayName":"Francesco Vitale","userId":"03232356734689144291"}}},"execution_count":3,"outputs":[],"id":"m3vrJgUDDbWM"},{"cell_type":"markdown","id":"3857d3e6","metadata":{"id":"3857d3e6"},"source":["# **Load the Data**"]},{"cell_type":"code","execution_count":4,"id":"a3c50560","metadata":{"id":"a3c50560","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1760545795951,"user_tz":-120,"elapsed":291438,"user":{"displayName":"Francesco Vitale","userId":"03232356734689144291"}},"outputId":"d2ddd301-01b5-4a98-c07a-0fb4c370fce5"},"outputs":[{"output_type":"stream","name":"stdout","text":["1 | V_0.1 -> Training: 75, Test: 48\n","1 | W_0.1 -> Training: 35, Test: 48\n","1 | V_0.2 -> Training: 72, Test: 48\n","1 | W_0.2 -> Training: 33, Test: 48\n","1 | V_0.3 -> Training: 69, Test: 48\n","1 | W_0.3 -> Training: 31, Test: 48\n","1 | V_0.4 -> Training: 66, Test: 48\n","1 | W_0.4 -> Training: 31, Test: 48\n","1 | V_0.5 -> Training: 63, Test: 48\n","1 | W_0.5 -> Training: 29, Test: 48\n","1 | V_0.6 -> Training: 59, Test: 48\n","1 | W_0.6 -> Training: 27, Test: 48\n","1 | V_0.7 -> Training: 56, Test: 48\n","1 | W_0.7 -> Training: 27, Test: 48\n","1 | V_0.8 -> Training: 53, Test: 48\n","1 | W_0.8 -> Training: 25, Test: 48\n","1 | V_0.9 -> Training: 50, Test: 48\n","1 | W_0.9 -> Training: 23, Test: 48\n","1 | V_1.0 -> Training: 48, Test: 48\n","1 | W_1.0 -> Training: 23, Test: 48\n","2 | V_0.1 -> Training: 75, Test: 48\n","2 | W_0.1 -> Training: 35, Test: 48\n","2 | V_0.2 -> Training: 72, Test: 48\n","2 | W_0.2 -> Training: 33, Test: 48\n","2 | V_0.3 -> Training: 69, Test: 48\n","2 | W_0.3 -> Training: 31, Test: 48\n","2 | V_0.4 -> Training: 66, Test: 48\n","2 | W_0.4 -> Training: 31, Test: 48\n","2 | V_0.5 -> Training: 63, Test: 48\n","2 | W_0.5 -> Training: 29, Test: 48\n","2 | V_0.6 -> Training: 59, Test: 48\n","2 | W_0.6 -> Training: 27, Test: 48\n","2 | V_0.7 -> Training: 56, Test: 48\n","2 | W_0.7 -> Training: 27, Test: 48\n","2 | V_0.8 -> Training: 53, Test: 48\n","2 | W_0.8 -> Training: 25, Test: 48\n","2 | V_0.9 -> Training: 50, Test: 48\n","2 | W_0.9 -> Training: 23, Test: 48\n","2 | V_1.0 -> Training: 48, Test: 48\n","2 | W_1.0 -> Training: 23, Test: 48\n","3 | V_0.1 -> Training: 75, Test: 48\n","3 | W_0.1 -> Training: 35, Test: 48\n","3 | V_0.2 -> Training: 72, Test: 48\n","3 | W_0.2 -> Training: 33, Test: 48\n","3 | V_0.3 -> Training: 69, Test: 48\n","3 | W_0.3 -> Training: 31, Test: 48\n","3 | V_0.4 -> Training: 66, Test: 48\n","3 | W_0.4 -> Training: 31, Test: 48\n","3 | V_0.5 -> Training: 63, Test: 48\n","3 | W_0.5 -> Training: 29, Test: 48\n","3 | V_0.6 -> Training: 59, Test: 48\n","3 | W_0.6 -> Training: 27, Test: 48\n","3 | V_0.7 -> Training: 56, Test: 48\n","3 | W_0.7 -> Training: 27, Test: 48\n","3 | V_0.8 -> Training: 53, Test: 48\n","3 | W_0.8 -> Training: 25, Test: 48\n","3 | V_0.9 -> Training: 50, Test: 48\n","3 | W_0.9 -> Training: 23, Test: 48\n","3 | V_1.0 -> Training: 48, Test: 48\n","3 | W_1.0 -> Training: 23, Test: 48\n"]}],"source":["def load_csv(path, drop_cols=None):\n","    df = pd.read_csv(path)\n","    if drop_cols:\n","        df = df.drop(drop_cols, axis=1)\n","    return df\n","\n","data = {}\n","drop_cols = ['C2']  # columns to drop\n","\n","# Helper to extract numeric parameter (accuracy) from key like \"V_0.1\"\n","def extract_acc(name):\n","    try:\n","        return float(name.split(\"_\")[1])\n","    except:\n","        return 0.0  # fallback\n","\n","for rep_number in os.listdir(input_dir):\n","    data[rep_number] = {}\n","\n","    rep_path = os.path.join(input_dir, rep_number)\n","    # sort keys numerically by <acc>\n","    keys = sorted(os.listdir(rep_path), key=extract_acc)\n","\n","    for key in keys:\n","        anomaly_type, parameter = key.split(\"_\")\n","\n","        data[rep_number][key] = {\"Training\": [], \"Test\": []}\n","\n","        # -------------------\n","        # Training data\n","        # -------------------\n","        training_dir = os.path.join(rep_path, key, \"TrainingTimeSeries\")\n","        if os.path.exists(training_dir):\n","            training_files = sorted([os.path.join(training_dir, f) for f in os.listdir(training_dir)])\n","            with ThreadPoolExecutor() as executor:\n","                data[rep_number][key][\"Training\"] = list(executor.map(lambda p: load_csv(p, drop_cols), training_files))\n","\n","        # -------------------\n","        # Test data (positive windows)\n","        # -------------------\n","        test_dir = os.path.join(rep_path, key, \"TestTimeSeries\")\n","        pos_test = []\n","        if os.path.exists(test_dir):\n","            test_files = sorted([os.path.join(test_dir, f) for f in os.listdir(test_dir)])\n","            with ThreadPoolExecutor() as executor:\n","                pos_test = list(executor.map(lambda p: load_csv(p, drop_cols), test_files))\n","        data[rep_number][key][\"Test\"].extend([[df, 1] for df in pos_test])\n","\n","        # -------------------\n","        # Test data (negative windows from other anomaly type)\n","        # -------------------\n","        other_type = \"W\" if anomaly_type == \"V\" else \"V\"\n","        other_test_dir = os.path.join(rep_path, f\"{other_type}_{parameter}\", \"TestTimeSeries\")\n","        neg_test = []\n","        if os.path.exists(other_test_dir):\n","            other_test_files = sorted([os.path.join(other_test_dir, f) for f in os.listdir(other_test_dir)])\n","            with ThreadPoolExecutor() as executor:\n","                neg_test = list(executor.map(lambda p: load_csv(p, drop_cols), other_test_files))\n","        data[rep_number][key][\"Test\"].extend([[df, 0] for df in neg_test])\n","\n","        print(f\"{rep_number} | {key} -> Training: {len(data[rep_number][key]['Training'])}, Test: {len(data[rep_number][key]['Test'])}\")"]},{"cell_type":"markdown","source":["# **Train/test**"],"metadata":{"id":"0Gs_MHlfpnHO"},"id":"0Gs_MHlfpnHO"},{"cell_type":"code","source":["# Custom RMSE loss\n","class RMSELoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    def forward(self, y_pred, y_true):\n","        return torch.sqrt(torch.mean((y_pred - y_true) ** 2) + 1e-8)\n","\n","criterion_rmse = RMSELoss()\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# --------------------------\n","# Dataset class\n","# --------------------------\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, X):\n","        self.X = X\n","    def __len__(self):\n","        return len(self.X)\n","    def __getitem__(self, idx):\n","        return self.X[idx]\n","\n","# --------------------------\n","# Autoencoder classes\n","# --------------------------\n","class LSTMAutoencoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=64, num_layers=1):\n","        super().__init__()\n","        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers, batch_first=True)\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1)\n","        enc_out, _ = self.encoder(x)\n","        dec_out, _ = self.decoder(enc_out)\n","        return dec_out.permute(0, 2, 1)\n","\n","class BiLSTMAutoencoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=64, num_layers=1):\n","        super().__init__()\n","        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n","        self.decoder = nn.LSTM(2*hidden_dim, input_dim, num_layers, batch_first=True)\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1)\n","        enc_out, _ = self.encoder(x)\n","        dec_out, _ = self.decoder(enc_out)\n","        return dec_out.permute(0, 2, 1)\n","\n","class GRUAutoencoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=64, num_layers=1):\n","        super().__init__()\n","        self.encoder = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.decoder = nn.GRU(hidden_dim, input_dim, num_layers, batch_first=True)\n","    def forward(self, x):\n","        x = x.permute(0, 2, 1)\n","        enc_out, _ = self.encoder(x)\n","        dec_out, _ = self.decoder(enc_out)\n","        return dec_out.permute(0, 2, 1)\n","\n","# --------------------------\n","# Training/evaluation function\n","# --------------------------\n","def train_one_class_ae(model_class, X_train_tensor, X_test_tensor, y_test, name, epochs=50):\n","    train_ds = TimeSeriesDataset(X_train_tensor)\n","    train_dl = DataLoader(train_ds, batch_size=8, shuffle=True)\n","\n","    model = model_class(input_dim=X_train_tensor.shape[1]).to(device)\n","    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n","\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for xb in train_dl:\n","            xb = xb.to(device)\n","            opt.zero_grad()\n","            recon = model(xb)\n","            loss = criterion_rmse(recon, xb)\n","            loss.backward()\n","            opt.step()\n","            total_loss += loss.item() * xb.size(0)\n","        total_loss /= len(train_dl.dataset)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        recon_test = model(X_test_tensor.to(device))\n","        errors = torch.mean((recon_test - X_test_tensor.to(device))**2, dim=[1,2]).cpu().numpy()\n","\n","        recon_train = model(X_train_tensor.to(device))\n","        train_errors = torch.mean((recon_train - X_train_tensor.to(device))**2, dim=[1,2]).cpu().numpy()\n","\n","    threshold = np.mean(train_errors) + np.std(train_errors)\n","    y_pred = (errors > threshold).astype(int)\n","    y_pred_corrected = 1 - y_pred\n","    f1 = f1_score(y_test, y_pred_corrected)\n","    return f1\n","\n","# --------------------------\n","# Pad/truncate helper\n","# --------------------------\n","def pad_truncate(arrays, target_len):\n","    res = []\n","    for x in arrays:\n","        T, C = x.shape\n","        if T < target_len:\n","            pad = np.zeros((target_len - T, C), dtype=np.float32)\n","            x_new = np.vstack([x, pad])\n","        else:\n","            x_new = x[:target_len, :]\n","        res.append(x_new)\n","    return np.stack(res)\n","\n","# --------------------------\n","# Main evaluation loop\n","# --------------------------\n","results = {}\n","\n","for rep_number in sorted(data.keys()):\n","    print(f\"\\nProcessing repetition: {rep_number}\")\n","    results.setdefault(rep_number, {})\n","\n","    for key in sorted(data[rep_number].keys()):\n","        print(f\"\\nDataset key: {key}\")\n","        training_time_series = data[rep_number][key][\"Training\"]\n","        test_time_series = data[rep_number][key][\"Test\"]\n","\n","        # Prepare train/test arrays\n","        X_train = [df.values.astype(np.float32) for df in training_time_series]\n","        X_test = [df.values.astype(np.float32) for df, _ in test_time_series]\n","        y_test = np.array([label for _, label in test_time_series], dtype=int)\n","\n","        # Determine uniform sequence length\n","        all_lengths = [x.shape[0] for x in X_train + X_test]\n","        seq_len = int(np.median(all_lengths))\n","\n","        # Pad/truncate\n","        X_train = pad_truncate(X_train, seq_len)\n","        X_test = pad_truncate(X_test, seq_len)\n","\n","        # Convert to PyTorch tensors\n","        X_train_tensor = torch.tensor(np.transpose(X_train, (0, 2, 1)), dtype=torch.float32)\n","        X_test_tensor = torch.tensor(np.transpose(X_test, (0, 2, 1)), dtype=torch.float32)\n","\n","        # Train and evaluate models\n","        f1_scores = {}\n","        for model_name, model_class in [(\"LSTM\", LSTMAutoencoder),\n","                                        (\"BiLSTM\", BiLSTMAutoencoder),\n","                                        (\"GRU\", GRUAutoencoder)]:\n","            f1 = train_one_class_ae(model_class, X_train_tensor, X_test_tensor, y_test, model_name)\n","            print(f\"  {model_name} F1: {f1:.4f}\")\n","            f1_scores.setdefault(model_name, []).append(f1)\n","\n","        # Store results per repetition\n","        results[rep_number][key] = f1_scores\n","\n","# --------------------------\n","# Compute mean ± std per key across repetitions and save\n","# --------------------------\n","with open(output_dir + \"f1_results.txt\", \"w\") as f:\n","    for key in sorted({k for rep in results.values() for k in rep.keys()}):\n","        f.write(f\"{key}:\\n\")\n","        print(f\"\\nSummary for {key}:\")\n","        for model_name in [\"LSTM\", \"BiLSTM\", \"GRU\"]:\n","            scores = [results[rep][key][model_name][0] for rep in results if key in results[rep]]\n","            mean_f1 = np.mean(scores)\n","            std_f1 = np.std(scores)\n","            f.write(f\"{model_name}: {mean_f1:.4f} ± {std_f1:.4f}\\n\")\n","            print(f\"  {model_name}: {mean_f1:.4f} ± {std_f1:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X4Nuaa4cpqBq","executionInfo":{"status":"ok","timestamp":1760546458114,"user_tz":-120,"elapsed":158678,"user":{"displayName":"Francesco Vitale","userId":"03232356734689144291"}},"outputId":"3b6dbc1e-1863-4bb7-e16d-6a8573f49470"},"id":"X4Nuaa4cpqBq","execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Processing repetition: 1\n","\n","Dataset key: V_0.1\n","  LSTM F1: 0.0588\n","  BiLSTM F1: 0.1667\n","  GRU F1: 0.6383\n","\n","Dataset key: V_0.2\n","  LSTM F1: 0.4255\n","  BiLSTM F1: 0.4000\n","  GRU F1: 0.2703\n","\n","Dataset key: V_0.3\n","  LSTM F1: 0.6038\n","  BiLSTM F1: 0.6087\n","  GRU F1: 0.6939\n","\n","Dataset key: V_0.4\n","  LSTM F1: 0.6429\n","  BiLSTM F1: 0.7451\n","  GRU F1: 0.4167\n","\n","Dataset key: V_0.5\n","  LSTM F1: 0.8000\n","  BiLSTM F1: 0.6939\n","  GRU F1: 0.8148\n","\n","Dataset key: V_0.6\n","  LSTM F1: 0.8358\n","  BiLSTM F1: 0.7451\n","  GRU F1: 0.7778\n","\n","Dataset key: V_0.7\n","  LSTM F1: 0.8406\n","  BiLSTM F1: 0.8852\n","  GRU F1: 0.8615\n","\n","Dataset key: V_0.8\n","  LSTM F1: 0.8696\n","  BiLSTM F1: 0.8070\n","  GRU F1: 0.9062\n","\n","Dataset key: V_0.9\n","  LSTM F1: 0.9538\n","  BiLSTM F1: 0.8421\n","  GRU F1: 0.8667\n","\n","Dataset key: V_1.0\n","  LSTM F1: 0.9677\n","  BiLSTM F1: 0.9355\n","  GRU F1: 0.9508\n","\n","Dataset key: W_0.1\n","  LSTM F1: 0.3214\n","  BiLSTM F1: 0.2909\n","  GRU F1: 0.1633\n","\n","Dataset key: W_0.2\n","  LSTM F1: 0.4262\n","  BiLSTM F1: 0.4667\n","  GRU F1: 0.3214\n","\n","Dataset key: W_0.3\n","  LSTM F1: 0.5085\n","  BiLSTM F1: 0.3509\n","  GRU F1: 0.3729\n","\n","Dataset key: W_0.4\n","  LSTM F1: 0.5263\n","  BiLSTM F1: 0.4333\n","  GRU F1: 0.3729\n","\n","Dataset key: W_0.5\n","  LSTM F1: 0.5357\n","  BiLSTM F1: 0.5085\n","  GRU F1: 0.3860\n","\n","Dataset key: W_0.6\n","  LSTM F1: 0.5769\n","  BiLSTM F1: 0.4407\n","  GRU F1: 0.4667\n","\n","Dataset key: W_0.7\n","  LSTM F1: 0.6275\n","  BiLSTM F1: 0.5263\n","  GRU F1: 0.4364\n","\n","Dataset key: W_0.8\n","  LSTM F1: 0.6400\n","  BiLSTM F1: 0.6154\n","  GRU F1: 0.5714\n","\n","Dataset key: W_0.9\n","  LSTM F1: 0.7027\n","  BiLSTM F1: 0.8421\n","  GRU F1: 0.6957\n","\n","Dataset key: W_1.0\n","  LSTM F1: 0.8824\n","  BiLSTM F1: 0.7778\n","  GRU F1: 0.9143\n","\n","Processing repetition: 2\n","\n","Dataset key: V_0.1\n","  LSTM F1: 0.3256\n","  BiLSTM F1: 0.1176\n","  GRU F1: 0.1622\n","\n","Dataset key: V_0.2\n","  LSTM F1: 0.5614\n","  BiLSTM F1: 0.4762\n","  GRU F1: 0.7925\n","\n","Dataset key: V_0.3\n","  LSTM F1: 0.5965\n","  BiLSTM F1: 0.7451\n","  GRU F1: 0.8148\n","\n","Dataset key: V_0.4\n","  LSTM F1: 0.7812\n","  BiLSTM F1: 0.8667\n","  GRU F1: 0.7407\n","\n","Dataset key: V_0.5\n","  LSTM F1: 0.7812\n","  BiLSTM F1: 0.8571\n","  GRU F1: 0.8772\n","\n","Dataset key: V_0.6\n","  LSTM F1: 0.8529\n","  BiLSTM F1: 0.8923\n","  GRU F1: 0.8070\n","\n","Dataset key: V_0.7\n","  LSTM F1: 0.8611\n","  BiLSTM F1: 0.8571\n","  GRU F1: 0.8254\n","\n","Dataset key: V_0.8\n","  LSTM F1: 0.8125\n","  BiLSTM F1: 0.8485\n","  GRU F1: 0.7541\n","\n","Dataset key: V_0.9\n","  LSTM F1: 0.8060\n","  BiLSTM F1: 0.8485\n","  GRU F1: 0.8525\n","\n","Dataset key: V_1.0\n","  LSTM F1: 0.7879\n","  BiLSTM F1: 0.9333\n","  GRU F1: 0.9000\n","\n","Dataset key: W_0.1\n","  LSTM F1: 0.5333\n","  BiLSTM F1: 0.3214\n","  GRU F1: 0.2857\n","\n","Dataset key: W_0.2\n","  LSTM F1: 0.5161\n","  BiLSTM F1: 0.3448\n","  GRU F1: 0.2264\n","\n","Dataset key: W_0.3\n","  LSTM F1: 0.5079\n","  BiLSTM F1: 0.4516\n","  GRU F1: 0.2593\n","\n","Dataset key: W_0.4\n","  LSTM F1: 0.5714\n","  BiLSTM F1: 0.4590\n","  GRU F1: 0.5079\n","\n","Dataset key: W_0.5\n","  LSTM F1: 0.5079\n","  BiLSTM F1: 0.5079\n","  GRU F1: 0.5000\n","\n","Dataset key: W_0.6\n","  LSTM F1: 0.5333\n","  BiLSTM F1: 0.4667\n","  GRU F1: 0.3158\n","\n","Dataset key: W_0.7\n","  LSTM F1: 0.6038\n","  BiLSTM F1: 0.4667\n","  GRU F1: 0.4918\n","\n","Dataset key: W_0.8\n","  LSTM F1: 0.6275\n","  BiLSTM F1: 0.6038\n","  GRU F1: 0.5517\n","\n","Dataset key: W_0.9\n","  LSTM F1: 0.8205\n","  BiLSTM F1: 0.7805\n","  GRU F1: 0.6275\n","\n","Dataset key: W_1.0\n","  LSTM F1: 0.8421\n","  BiLSTM F1: 0.7805\n","  GRU F1: 0.6667\n","\n","Processing repetition: 3\n","\n","Dataset key: V_0.1\n","  LSTM F1: 0.2326\n","  BiLSTM F1: 0.2222\n","  GRU F1: 0.3158\n","\n","Dataset key: V_0.2\n","  LSTM F1: 0.4727\n","  BiLSTM F1: 0.3500\n","  GRU F1: 0.7059\n","\n","Dataset key: V_0.3\n","  LSTM F1: 0.5517\n","  BiLSTM F1: 0.7500\n","  GRU F1: 0.4783\n","\n","Dataset key: V_0.4\n","  LSTM F1: 0.5000\n","  BiLSTM F1: 0.4000\n","  GRU F1: 0.6792\n","\n","Dataset key: V_0.5\n","  LSTM F1: 0.6452\n","  BiLSTM F1: 0.7719\n","  GRU F1: 0.6885\n","\n","Dataset key: V_0.6\n","  LSTM F1: 0.7576\n","  BiLSTM F1: 0.8710\n","  GRU F1: 0.8136\n","\n","Dataset key: V_0.7\n","  LSTM F1: 0.7826\n","  BiLSTM F1: 0.7812\n","  GRU F1: 0.7879\n","\n","Dataset key: V_0.8\n","  LSTM F1: 0.7826\n","  BiLSTM F1: 0.8235\n","  GRU F1: 0.8358\n","\n","Dataset key: V_0.9\n","  LSTM F1: 0.8571\n","  BiLSTM F1: 0.9231\n","  GRU F1: 0.8696\n","\n","Dataset key: V_1.0\n","  LSTM F1: 0.9355\n","  BiLSTM F1: 0.9677\n","  GRU F1: 0.9333\n","\n","Dataset key: W_0.1\n","  LSTM F1: 0.5085\n","  BiLSTM F1: 0.3214\n","  GRU F1: 0.3793\n","\n","Dataset key: W_0.2\n","  LSTM F1: 0.5517\n","  BiLSTM F1: 0.4918\n","  GRU F1: 0.5000\n","\n","Dataset key: W_0.3\n","  LSTM F1: 0.5085\n","  BiLSTM F1: 0.4333\n","  GRU F1: 0.5161\n","\n","Dataset key: W_0.4\n","  LSTM F1: 0.5161\n","  BiLSTM F1: 0.4762\n","  GRU F1: 0.5246\n","\n","Dataset key: W_0.5\n","  LSTM F1: 0.6038\n","  BiLSTM F1: 0.5161\n","  GRU F1: 0.5161\n","\n","Dataset key: W_0.6\n","  LSTM F1: 0.6400\n","  BiLSTM F1: 0.5926\n","  GRU F1: 0.4590\n","\n","Dataset key: W_0.7\n","  LSTM F1: 0.6275\n","  BiLSTM F1: 0.6400\n","  GRU F1: 0.5161\n","\n","Dataset key: W_0.8\n","  LSTM F1: 0.6531\n","  BiLSTM F1: 0.6250\n","  GRU F1: 0.6154\n","\n","Dataset key: W_0.9\n","  LSTM F1: 0.7429\n","  BiLSTM F1: 0.7027\n","  GRU F1: 0.7111\n","\n","Dataset key: W_1.0\n","  LSTM F1: 0.8000\n","  BiLSTM F1: 0.9697\n","  GRU F1: 0.9143\n","\n","Summary for V_0.1:\n","  LSTM: 0.2057 ± 0.1106\n","  BiLSTM: 0.1688 ± 0.0427\n","  GRU: 0.3721 ± 0.1984\n","\n","Summary for V_0.2:\n","  LSTM: 0.4866 ± 0.0563\n","  BiLSTM: 0.4087 ± 0.0519\n","  GRU: 0.5895 ± 0.2285\n","\n","Summary for V_0.3:\n","  LSTM: 0.5840 ± 0.0230\n","  BiLSTM: 0.7013 ± 0.0655\n","  GRU: 0.6623 ± 0.1392\n","\n","Summary for V_0.4:\n","  LSTM: 0.6414 ± 0.1148\n","  BiLSTM: 0.6706 ± 0.1977\n","  GRU: 0.6122 ± 0.1405\n","\n","Summary for V_0.5:\n","  LSTM: 0.7421 ± 0.0690\n","  BiLSTM: 0.7743 ± 0.0667\n","  GRU: 0.7935 ± 0.0785\n","\n","Summary for V_0.6:\n","  LSTM: 0.8154 ± 0.0415\n","  BiLSTM: 0.8361 ± 0.0650\n","  GRU: 0.7995 ± 0.0156\n","\n","Summary for V_0.7:\n","  LSTM: 0.8281 ± 0.0332\n","  BiLSTM: 0.8412 ± 0.0439\n","  GRU: 0.8249 ± 0.0301\n","\n","Summary for V_0.8:\n","  LSTM: 0.8216 ± 0.0361\n","  BiLSTM: 0.8263 ± 0.0170\n","  GRU: 0.8321 ± 0.0622\n","\n","Summary for V_0.9:\n","  LSTM: 0.8723 ± 0.0613\n","  BiLSTM: 0.8712 ± 0.0368\n","  GRU: 0.8629 ± 0.0075\n","\n","Summary for V_1.0:\n","  LSTM: 0.8970 ± 0.0783\n","  BiLSTM: 0.9455 ± 0.0157\n","  GRU: 0.9281 ± 0.0211\n","\n","Summary for W_0.1:\n","  LSTM: 0.4544 ± 0.0946\n","  BiLSTM: 0.3113 ± 0.0144\n","  GRU: 0.2761 ± 0.0885\n","\n","Summary for W_0.2:\n","  LSTM: 0.4980 ± 0.0528\n","  BiLSTM: 0.4344 ± 0.0642\n","  GRU: 0.3493 ± 0.1134\n","\n","Summary for W_0.3:\n","  LSTM: 0.5083 ± 0.0003\n","  BiLSTM: 0.4119 ± 0.0438\n","  GRU: 0.3828 ± 0.1051\n","\n","Summary for W_0.4:\n","  LSTM: 0.5380 ± 0.0240\n","  BiLSTM: 0.4562 ± 0.0176\n","  GRU: 0.4685 ± 0.0679\n","\n","Summary for W_0.5:\n","  LSTM: 0.5491 ± 0.0403\n","  BiLSTM: 0.5108 ± 0.0037\n","  GRU: 0.4674 ± 0.0579\n","\n","Summary for W_0.6:\n","  LSTM: 0.5834 ± 0.0438\n","  BiLSTM: 0.5000 ± 0.0663\n","  GRU: 0.4138 ± 0.0694\n","\n","Summary for W_0.7:\n","  LSTM: 0.6196 ± 0.0112\n","  BiLSTM: 0.5443 ± 0.0719\n","  GRU: 0.4814 ± 0.0334\n","\n","Summary for W_0.8:\n","  LSTM: 0.6402 ± 0.0105\n","  BiLSTM: 0.6147 ± 0.0087\n","  GRU: 0.5795 ± 0.0266\n","\n","Summary for W_0.9:\n","  LSTM: 0.7554 ± 0.0489\n","  BiLSTM: 0.7751 ± 0.0570\n","  GRU: 0.6781 ± 0.0363\n","\n","Summary for W_1.0:\n","  LSTM: 0.8415 ± 0.0336\n","  BiLSTM: 0.8427 ± 0.0898\n","  GRU: 0.8317 ± 0.1167\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"collapsed_sections":["gAptdhizPEir"],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}